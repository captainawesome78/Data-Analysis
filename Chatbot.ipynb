{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2576054e04e8437eb04481da796cc0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0c04d7967204f24b3536ddfb244e6a9",
              "IPY_MODEL_8fe8175fb80540238a63e1e8554b1e02",
              "IPY_MODEL_28bae53f662b44d5bc56cca040b1c65e"
            ],
            "layout": "IPY_MODEL_e1515fe0f08e4bddabc258b4b27dd42e"
          }
        },
        "f0c04d7967204f24b3536ddfb244e6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa099b39b1e14c40b3e2f0041337ad0d",
            "placeholder": "​",
            "style": "IPY_MODEL_f066583b361941cfbee875441705b44e",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "8fe8175fb80540238a63e1e8554b1e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88584b9fa613424694ee988135006dc0",
            "max": 644,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed6b507e737d45a3829ea2f0d1f25024",
            "value": 644
          }
        },
        "28bae53f662b44d5bc56cca040b1c65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6db669cd2a54aca9ba3174e378b0fa6",
            "placeholder": "​",
            "style": "IPY_MODEL_23c57a1993e34ee3b4e470e904c29657",
            "value": " 644/644 [00:00&lt;00:00, 34.1kB/s]"
          }
        },
        "e1515fe0f08e4bddabc258b4b27dd42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa099b39b1e14c40b3e2f0041337ad0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f066583b361941cfbee875441705b44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88584b9fa613424694ee988135006dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6b507e737d45a3829ea2f0d1f25024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6db669cd2a54aca9ba3174e378b0fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c57a1993e34ee3b4e470e904c29657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64ff331f1b9249398cda6461653c7a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed2a64d19fa8455fb7aaf7464e73b792",
              "IPY_MODEL_7ca60848e3564e67a5b6e85b1efdf1ae",
              "IPY_MODEL_0eede08c9cac4463a91a3472582cf1e4"
            ],
            "layout": "IPY_MODEL_bc3dc79613f043f78e5c3a2895f1e12f"
          }
        },
        "ed2a64d19fa8455fb7aaf7464e73b792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fd01a0f87a94719802161a62af9a19a",
            "placeholder": "​",
            "style": "IPY_MODEL_2952fd081d234dad9a27245035985c6a",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "7ca60848e3564e67a5b6e85b1efdf1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_167dff9af43f46efa64853e8e2e33221",
            "max": 662513657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d79c45aba09b4e9ea9cc92bf90cf9129",
            "value": 662513657
          }
        },
        "0eede08c9cac4463a91a3472582cf1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ac8deb7ee4403395e4cf144d6c2ca5",
            "placeholder": "​",
            "style": "IPY_MODEL_2832f937861649959b7d2c6b58647f31",
            "value": " 663M/663M [00:09&lt;00:00, 78.4MB/s]"
          }
        },
        "bc3dc79613f043f78e5c3a2895f1e12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd01a0f87a94719802161a62af9a19a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2952fd081d234dad9a27245035985c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "167dff9af43f46efa64853e8e2e33221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79c45aba09b4e9ea9cc92bf90cf9129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54ac8deb7ee4403395e4cf144d6c2ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2832f937861649959b7d2c6b58647f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_erL0d01Ea4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6addddbf-fc24-4c99-b687-e19c4e53c5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.8.1-py3-none-any.whl (1.4 MB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.0.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Collecting msg-parser (from unstructured)\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.5.3)\n",
            "Collecting pdf2image (from unstructured)\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pdfminer.six (from unstructured)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unstructured) (8.4.0)\n",
            "Collecting pypandoc (from unstructured)\n",
            "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Collecting python-docx (from unstructured)\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-pptx (from unstructured)\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.0.1)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured)\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.65.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured) (1.22.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured) (2.0.12)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured)\n",
            "  Downloading cryptography-41.0.2-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx->unstructured)\n",
            "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured) (1.15.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->unstructured) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured) (2.21)\n",
            "Building wheels for collected packages: python-docx, python-pptx, olefile\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184491 sha256=88f15cceb735b7ba84a4471430c45bae03e0520c6f79d719535151c3f6b15b67\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470935 sha256=9b5b86cdf50536901619ead65b68441706de513d66e6356f7d292d80b36d6810\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=571a2343f7724976fc10d8655e6792424b266dc7f7817c2e5b1191d16a4c06ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "Successfully built python-docx python-pptx olefile\n",
            "Installing collected packages: filetype, XlsxWriter, python-magic, python-docx, pypandoc, pdf2image, olefile, python-pptx, msg-parser, cryptography, pdfminer.six, unstructured\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 3.4.8\n",
            "    Uninstalling cryptography-3.4.8:\n",
            "      Successfully uninstalled cryptography-3.4.8\n",
            "Successfully installed XlsxWriter-3.1.2 cryptography-41.0.2 filetype-1.2.0 msg-parser-1.2.0 olefile-0.46 pdf2image-1.16.3 pdfminer.six-20221105 pypandoc-1.11 python-docx-0.8.11 python-magic-0.4.27 python-pptx-0.6.21 unstructured-0.8.1\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.2-py3-none-any.whl (399 kB)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.11)\n",
            "Collecting chroma-hnswlib==0.7.1 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=2273167 sha256=1556c8d8cba216bd068a5f0cb6a3e3f255540c637cf42d370fe979e6077a55a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=54da3d37a9244427525064ec2003d3186af1264f7a621b25e16e987515e87138\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.2 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.15.1 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 pypika-0.48.9 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 uvicorn-0.23.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --qqq\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.30.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.21.0 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.233 --progress-bar off\n",
        "!pip install unstructured --progress-bar off\n",
        "!pip install chromadb --progress-bar off\n",
        "!pip install --qqq sentence_transformers --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.schema import BaseOutputParser\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    pipeline,\n",
        "    GenerationConfig,\n",
        "    TextStreamer\n",
        ")"
      ],
      "metadata": {
        "id": "4vOtcw-bEqD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0193f99b-02e3-40fb-f754-343f6c429d5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "UVY1crDOdshk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model"
      ],
      "metadata": {
        "id": "JAnIY1hEG0sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"facebook/opt-350m\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, trust_remote_code=True, load_in_4bit=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "pRJbhBh5G0KZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578,
          "referenced_widgets": [
            "2576054e04e8437eb04481da796cc0ff",
            "f0c04d7967204f24b3536ddfb244e6a9",
            "8fe8175fb80540238a63e1e8554b1e02",
            "28bae53f662b44d5bc56cca040b1c65e",
            "e1515fe0f08e4bddabc258b4b27dd42e",
            "aa099b39b1e14c40b3e2f0041337ad0d",
            "f066583b361941cfbee875441705b44e",
            "88584b9fa613424694ee988135006dc0",
            "ed6b507e737d45a3829ea2f0d1f25024",
            "e6db669cd2a54aca9ba3174e378b0fa6",
            "23c57a1993e34ee3b4e470e904c29657",
            "64ff331f1b9249398cda6461653c7a59",
            "ed2a64d19fa8455fb7aaf7464e73b792",
            "7ca60848e3564e67a5b6e85b1efdf1ae",
            "0eede08c9cac4463a91a3472582cf1e4",
            "bc3dc79613f043f78e5c3a2895f1e12f",
            "8fd01a0f87a94719802161a62af9a19a",
            "2952fd081d234dad9a27245035985c6a",
            "167dff9af43f46efa64853e8e2e33221",
            "d79c45aba09b4e9ea9cc92bf90cf9129",
            "54ac8deb7ee4403395e4cf144d6c2ca5",
            "2832f937861649959b7d2c6b58647f31"
          ]
        },
        "outputId": "71f4d0dd-ae32-418d-cc77-52fa2773de4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2576054e04e8437eb04481da796cc0ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64ff331f1b9249398cda6461653c7a59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6128c57797ba>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/opt-350m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2817\u001b[0m                 }\n\u001b[1;32m   2818\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2819\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   2820\u001b[0m                         \"\"\"\n\u001b[1;32m   2821\u001b[0m                         \u001b[0mSome\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdispatched\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCPU\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMake\u001b[0m \u001b[0msure\u001b[0m \u001b[0myou\u001b[0m \u001b[0mhave\u001b[0m \u001b[0menough\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mRAM\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config"
      ],
      "metadata": {
        "id": "x6lWtHEOS8PB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e81758-e75e-4973-f48e-387fc8d01492"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"_from_model_config\": true,\n",
              "  \"bos_token_id\": 2,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"transformers_version\": \"4.30.0\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we're loading the model in 8-bit mode. This will reduce the memory footprint and speed up the inference. We're also using the device_map parameter to load the model on the GPU."
      ],
      "metadata": {
        "id": "oh39CXWvRxxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small Inference"
      ],
      "metadata": {
        "id": "SpddSCgxTLfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question = (\"Which programming languages are more suited for beginners? \")\n",
        "\n",
        "# prompt = f\"\"\"\n",
        "#  ## Instruction : {Question}\n",
        "#  ## Response :\n",
        "#  \"\"\".strip()\n",
        "\n",
        "prompt = \"\"\"\n",
        "The following is a friendly conversation between a human and an AI. The AI is\n",
        "talkative and provides lots of specific details from its context.\n",
        "\n",
        "Current conversation:\n",
        "\n",
        "Human: Who is Spiderman?\n",
        "AI:\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "fnbqYZAoTN5J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt,return_tensors='pt').input_ids\n",
        "input_ids = input_ids.to(model.device)\n",
        "# with torch.inference_model(output = model.generate(input=input_ids, temperature = 0.7, max_new_tokens = 1000))\n",
        "output = model.generate(input_ids, temperature=0.7, max_new_tokens=1000)"
      ],
      "metadata": {
        "id": "CO-jIMQ2U1q8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uvVJscgfQB0",
        "outputId": "1b75cf76-d52a-42b3-ac5d-d64bfb2301f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    2,   133,   511,    16,    10,  5192,  1607,   227,    10,  1050,\n",
            "             8,    41,  4687,     4,    20,  4687,    16, 50118, 26594,  3693,\n",
            "             8,  1639,  3739,     9,  2167,  1254,    31,    63,  5377,     4,\n",
            "         50118,  1437, 50118, 42124,  1607,    35, 50118,  1437, 50118, 33837,\n",
            "            35,  3394,    16, 15749,   397,   116, 50118, 15238,    35]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "gFvoTIxdVZyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693c68bf-4579-4ac8-87fc-2caef0901dda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s>The following is a friendly conversation between a human and an AI. The AI is\n",
            "talkative and provides lots of specific details from its context.\n",
            " \n",
            "Current conversation:\n",
            " \n",
            "Human: Who is Spiderman?\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "AI: Spiderman.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config"
      ],
      "metadata": {
        "id": "pgqwbXvPWXYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9b4465-3701-4f8c-bdcd-95a4d8fe0ce7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"_from_model_config\": true,\n",
              "  \"bos_token_id\": 2,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"transformers_version\": \"4.30.0\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed the documents"
      ],
      "metadata": {
        "id": "_YryV6OVeqBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "import sentence_transformers\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name = \"intfloat/e5-base-v2\"\n",
        "# )\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "#     model_name = \"facebook/opt-350m\"\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# )\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name = \"facebook/opt-350m\"\n",
        "\n",
        "# )"
      ],
      "metadata": {
        "id": "9BxazAWwetR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "cbb8d3e1-0888-4c39-be0c-48ed6b0b36d9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c84b2fd7f354>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sentence_transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# embeddings = HuggingFaceEmbeddings(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     model_name = \"intfloat/e5-base-v2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a custom configuration for the text generation:"
      ],
      "metadata": {
        "id": "jvKNNxcnQxU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = model.generation_config\n",
        "generation_config.temperature = 0\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.max_new_tokens = 256\n",
        "generation_config.use_cache = False\n",
        "generation_config.repetition_penalty = 1.7\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config"
      ],
      "metadata": {
        "id": "6fZm5h0UQw-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the temperature to 0 to get deterministic results.  We'll also set the repetition_penalty to 1.7 to reduce the chance (but not completely remove the occurrences) of the model repeating itself."
      ],
      "metadata": {
        "id": "qo-dfFR2RShF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, use_multiprocessing=False)"
      ],
      "metadata": {
        "id": "IqG997FwRX3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying using the huggingface pipeline"
      ],
      "metadata": {
        "id": "6fmzDL66dCom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=2048,\n",
        "    temperature=0,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        "    generation_config=generation_config,\n",
        "    streamer=streamer,\n",
        "    batch_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "qN8tzgx2c9x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipepine(pipeline=pipe)"
      ],
      "metadata": {
        "id": "D837AcxVdkyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response= llm(prompt  )"
      ],
      "metadata": {
        "id": "LCJ_Yn1TdtgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "UNos8FwrkTo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path = \"/content/pdfs\"\n",
        "loader = UnstructuredPDFLoader(\"/content/pdfs/bd.pdf\")\n",
        "loader"
      ],
      "metadata": {
        "id": "sjHdYOmekTIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load_and_split()\n",
        "documents"
      ],
      "metadata": {
        "id": "2scJnd71kaUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size= 1000, chunk_overlap=20)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "q5ckFie5kc0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(texts,embeddings)"
      ],
      "metadata": {
        "id": "woCdKE04knA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# db.similarity_search()"
      ],
      "metadata": {
        "id": "T62MU7gQkv5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conversational Chain"
      ],
      "metadata": {
        "id": "oCONlcOwlmka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template= \"\"\"\n",
        "  You are a product analyst chatbot that is taling to product managers. use only the chat history and the following information\n",
        "  {context}\n",
        "  to answer in a helpful manner to the question. if you don't know the answer - say that you don't know.\n",
        "  keep your replies compassionate and informative\n",
        "  {chat history}\n",
        "  ## Input : {question}\n",
        "  ## Response :\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "wUIGUQU-lqlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\", \"chat_history\"], template = template)"
      ],
      "metadata": {
        "id": "EQ9k7N1om19W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    human_prefix=\"## Input\",\n",
        "    ai_prefix=\"## Response\",\n",
        "    output_key=\"answer\",\n",
        "    return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "_rAI81BZnNv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = ConversationalretrievalChain.from_llm(\n",
        "    llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(),\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\":prompt},\n",
        "    return_source_documents=True,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "gR7V-UZ0nt9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are rhe features of Brand Protector?\"\n",
        "answer = chain(question)"
      ],
      "metadata": {
        "id": "9FXPzpzaoOa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QA Chain with Memory"
      ],
      "metadata": {
        "id": "21RuFGVooujX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    human_prefix=\"## Input\",\n",
        "    ai_prefix=\"## Response\",\n",
        "    input_key=\"question\"\n",
        "    output_key=\"output_text\",\n",
        "    return_messages=False\n",
        ")"
      ],
      "metadata": {
        "id": "g5sX9UsQoyaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(\n",
        "    llm,\n",
        "    chain_type=\"stuff\",\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "4yFEqGAmpE-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are rhe features of Brand Protector?\"\n",
        "docs = db.similarity_search(question)\n",
        "answer = chain({\"input_documents\":docs, \"question\":question})"
      ],
      "metadata": {
        "id": "pYVzAEtlpgag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support Chatbot"
      ],
      "metadata": {
        "id": "gI7Qmsqp7Aw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_TEMPLATE = \"\"\"\n",
        "  ## Instruction : You are a product analyst chatbot that is taling to product managers. use only the chat history and the following information\n",
        "  {context}\n",
        "  to answer in a helpful manner to the question. if you don't know the answer - say that you don't know.\n",
        "  keep your replies compassionate and informative\n",
        "  {chat history}\n",
        "  ## Input : {question}\n",
        "  ## Response :\n",
        "\"\"\".strip()\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(\n",
        "        self,\n",
        "        text_pipeline: HuggingFacePipeline,\n",
        "        embeddings: HuggingFaceEmbeddings,\n",
        "        documents_dir:Path,\n",
        "        prompt_template: str = DEFAULT_TEMPLATE,\n",
        "        verbose: bool = True\n",
        "    ):\n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
        "            template=prompt_template,\n",
        "        )\n",
        "        self.chain = self._create_chain(text_pipeline, prompt, verbose)\n",
        "        self.db = self._embed_data(documents_dir, embeddings)\n",
        "\n",
        "    def _create_chain(\n",
        "        self,\n",
        "        text_pipeline: HuggingFacePipeline,\n",
        "        prompt= PromptTemplate,\n",
        "        verbose: bool = True:\n",
        "    ):\n",
        "        memory = ConversationBufferMemory(\n",
        "          memory_key=\"chat_history\",\n",
        "          human_prefix=\"## Input\",\n",
        "          ai_prefix=\"## Response\",\n",
        "          input_key=\"question\"\n",
        "          output_key=\"output_text\",\n",
        "          return_messages=False\n",
        "        )\n",
        "\n",
        "        return load_qa_chain(\n",
        "            text_pipeline,\n",
        "            chain_type = \"stuff\",\n",
        "            prompt=prompt,\n",
        "            memory=memory,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "    def _embed_data(\n",
        "        self, documents_dir: Path, embeddings: HuggingFaceEmbeddings\n",
        "    ) -> Chroma:\n",
        "        loader = loader = UnstructuredPDFLoader(\"/content/pdfs/bd.pdf\")\n",
        "        documents = loader.load_and_split()\n",
        "        text_splitter = CharacterTextSplitter(chunk_size= 1000, chunk_overlap=20)\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        return Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "\n",
        "    def __call__(self, user_input: str) -> str:\n",
        "      docs = self,db,similarity_search(user_input)\n",
        "      return self.chain.run({\"input_documents\": docs, \"question\": user_input})"
      ],
      "metadata": {
        "id": "ckWWpCUt7DVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = Chatbot(llm, embeddings,\"/content/pdfs/bd.pdf\")"
      ],
      "metadata": {
        "id": "P03OaIv1_T4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "while True:\n",
        "  user_input = input(\"You: \")\n",
        "  if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
        "      break\n",
        "  answer = chatbot(user_input)\n",
        "  print()"
      ],
      "metadata": {
        "id": "oG0G5qwd_eFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}